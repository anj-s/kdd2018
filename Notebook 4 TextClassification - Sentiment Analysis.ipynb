{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Keras and verify that the TensorFlow backend is set as the default.\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S/W < 1500 -> MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to gather data. The more training examples you\n",
    "# have the better you will be able to train a generalized mode.\n",
    "# You should also make sure that the samples for every class is not\n",
    "# imbalanced. There should be a fairly even representation of\n",
    "# all samples. To illustrate this workflow we will use the IMDB\n",
    "# dataset.\n",
    "data_path = '/Users/anjalisridhar/kdd2018/workshop/datasets'\n",
    "imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "seed = 123\n",
    "# Load the training data\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "    for fname in sorted(os.listdir(train_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(train_path, fname)) as f:\n",
    "                train_texts.append(f.read())\n",
    "            train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "# Load the validation data.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "    for fname in sorted(os.listdir(test_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(test_path, fname)) as f:\n",
    "                test_texts.append(f.read())\n",
    "            test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "# Shuffle the training data and labels. \n",
    "# The data gathered may be in a specific order and we should\n",
    "# shuffle the data before doing anything else.\n",
    "random.seed(seed)\n",
    "random.shuffle(train_texts)\n",
    "random.seed(seed)\n",
    "random.shuffle(train_labels)\n",
    "\n",
    "data = ((train_texts, np.array(train_labels)),\n",
    "        (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data.\n",
    "(train_texts, train_labels), (val_texts, val_labels) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that validation labels are in the same range as training labels.\n",
    "num_classes = max(train_labels) + 1\n",
    "missing_classes = [i for i in range(num_classes) if i not in train_labels]\n",
    "if len(missing_classes):\n",
    "    raise ValueError('Missing samples with label value(s) '\n",
    "                     '{missing_classes}. Please make sure you have '\n",
    "                     'at least one sample for every label value '\n",
    "                     'in the range(0, {max_class})'.format(\n",
    "                        missing_classes=missing_classes,\n",
    "                        max_class=num_classes - 1))\n",
    "\n",
    "if num_classes <= 1:\n",
    "    raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                     'Please make sure there are at least two classes '\n",
    "                     'of samples'.format(num_classes=num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "if len(unexpected_labels):\n",
    "    raise ValueError('Unexpected label values found in the validation set:'\n",
    "                     ' {unexpected_labels}. Please make sure that the '\n",
    "                     'labels in the validation set are in the same range '\n",
    "                     'as training labels.'.format(\n",
    "                         unexpected_labels=unexpected_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Vectorization\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "# Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "}\n",
    "# Tokenizing samples into unigrams + bigrams provides good accuracy\n",
    "# while taking less compute time.\n",
    "# We use Tf-idf encoding for vectorization. This does better than\n",
    "# one-hot encoding and count encoding in terms of accuracy\n",
    "# (on average: 0.25-15% higher). Tf-idf uses floating point \n",
    "# representation and takes more time to compute and uses more\n",
    "# memory.\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Vectorize validation texts.\n",
    "x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "# When we convert texts to tokens we may end up with a large\n",
    "# number of tokens. We want to drop rarely occurring tokens\n",
    "# as well as tokens that don't contribute heavily to label \n",
    "# predictions. \n",
    "# We use the `f_classif` function to identify the top 20K features.\n",
    "# Select top 'k' of the vectorized features.\n",
    "selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "selector.fit(x_train, train_labels)\n",
    "x_train = selector.transform(x_train)\n",
    "x_val = selector.transform(x_val)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance.\n",
    "learning_rate=1e-3,\n",
    "#TODO: Do I have to train epochs=1000?\n",
    "# Try for more epochs\n",
    "epochs=10\n",
    "batch_size=128\n",
    "layers=2\n",
    "units=64\n",
    "dropout_rate=0.2\n",
    "input_shape=x_train.shape[1:]\n",
    "\n",
    "if num_classes == 2:\n",
    "    op_activation = 'sigmoid'\n",
    "    op_units = 1\n",
    "else:\n",
    "    op_activation = 'softmax'\n",
    "    op_units = num_classes\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "for _ in range(layers-1):\n",
    "    model.add(keras.layers.Dense(units=units, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "model.add(keras.layers.Dense(units=op_units, activation=op_activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with learning parameters.\n",
    "if num_classes == 2:\n",
    "    loss = 'binary_crossentropy'\n",
    "else:\n",
    "    loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 0.4709 - acc: 0.8537 - val_loss: 0.3227 - val_acc: 0.8862\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.2315 - acc: 0.9229 - val_loss: 0.2504 - val_acc: 0.9020\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.1690 - acc: 0.9415 - val_loss: 0.2349 - val_acc: 0.9055\n",
      "Epoch 4/10\n",
      " - 10s - loss: 0.1349 - acc: 0.9550 - val_loss: 0.2296 - val_acc: 0.9058\n",
      "Epoch 5/10\n",
      " - 10s - loss: 0.1118 - acc: 0.9636 - val_loss: 0.2323 - val_acc: 0.9041\n",
      "Epoch 6/10\n",
      " - 10s - loss: 0.0946 - acc: 0.9701 - val_loss: 0.2403 - val_acc: 0.9015\n",
      "Validation accuracy: 0.901519999981, loss: 0.240315819798\n",
      "(0.9015199999809265, 0.24031581979751587)\n"
     ]
    }
   ],
   "source": [
    "# Create callback for early stopping on validation loss. If the loss does\n",
    "# not decrease in two consecutive tries, stop training.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=2)]\n",
    "\n",
    "# Train and validate model.\n",
    "history = model.fit(\n",
    "        x_train,\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(x_val, val_labels),\n",
    "        verbose=2,  # Logs once per epoch.\n",
    "        batch_size=batch_size)\n",
    "\n",
    "# Print results.\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "# Save model.\n",
    "model.save('imdb_mlp_model.h5')\n",
    "print(history['val_acc'][-1], history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S/W > 1500 -> sepCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, zipfile\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = (2, 3)  # 2 - Phrases, 3 - Sentiment.\n",
    "data_path = '/Users/anjalisridhar/kdd2018/workshop/datasets'\n",
    "file_name = 'rotten_tomatoes_train.tsv'\n",
    "seed = 123\n",
    "validation_split = 0.2\n",
    "separator = '\\t'\n",
    "header = 0\n",
    "\n",
    "# Using the Rotten tomatoes movie reviews dataset to demonstrate\n",
    "# training sequence model.\n",
    "np.random.seed(seed)\n",
    "data_path = os.path.join(data_path, file_name)\n",
    "data = pd.read_csv(data_path, usecols=columns, sep=separator, header=header)\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "# Get the review phrase and sentiment values.\n",
    "texts = list(data['Phrase'])\n",
    "labels = np.array(data['Sentiment'])\n",
    "num_training_samples = int((1 - validation_split) * len(texts))\n",
    "data = ((texts[:num_training_samples], labels[:num_training_samples]),\n",
    "       (texts[num_training_samples:], labels[num_training_samples:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "epochs=1000\n",
    "batch_size=128\n",
    "blocks=2\n",
    "filters=64\n",
    "dropout_rate=0.2\n",
    "embedding_dim=200\n",
    "kernel_size=3\n",
    "pool_size=3\n",
    "\n",
    "(train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "# Verify that validation labels are in the same range as training labels.\n",
    "num_classes = max(train_labels) + 1\n",
    "missing_classes = [i for i in range(num_classes) if i not in train_labels]\n",
    "if len(missing_classes):\n",
    "    raise ValueError('Missing samples with label value(s) '\n",
    "                     '{missing_classes}. Please make sure you have '\n",
    "                     'at least one sample for every label value '\n",
    "                     'in the range(0, {max_class})'.format(\n",
    "                        missing_classes=missing_classes,\n",
    "                        max_class=num_classes - 1))\n",
    "\n",
    "if num_classes <= 1:\n",
    "    raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                     'Please make sure there are at least two classes '\n",
    "                     'of samples'.format(num_classes=num_classes))\n",
    "unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "if len(unexpected_labels):\n",
    "      raise ValueError('Unexpected label values found in the validation set:'\n",
    "                       ' {unexpected_labels}. Please make sure that the '\n",
    "                       'labels in the validation set are in the same range '\n",
    "                       'as training labels.'.format(\n",
    "                           unexpected_labels=unexpected_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorize texts.\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# We need to convert our text samples into numerical vectors.\n",
    "# We first build a vocabulary of the 20K most frequently occurring\n",
    "# words. Each word in the vocab is associated with an index.\n",
    "# Create vocabulary with training texts.\n",
    "tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Vectorize training and validation texts.\n",
    "x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "# Get max sequence length.\n",
    "max_length = len(max(x_train, key=len))\n",
    "if max_length > MAX_SEQUENCE_LENGTH:\n",
    "    max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "# Fix sequence length to max value. Sequences shorter than the length are\n",
    "# padded in the beginning and sequences longer are truncated\n",
    "# at the beginning.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Number of features will be the embedding input dimension. Add 1 for the\n",
    "# reserved index 0.\n",
    "num_features = min(len(word_index) + 1, TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SeparableConv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "# Create model instance.\n",
    "input_shape=x_train.shape[1:]\n",
    "use_pretrained_embedding=False\n",
    "is_embedding_trainable=False\n",
    "embedding_matrix=None\n",
    "\n",
    "if num_classes == 2:\n",
    "  op_activation = 'sigmoid'\n",
    "  op_units = 1\n",
    "else:\n",
    "  op_activation = 'softmax'\n",
    "  op_units = num_classes\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "# embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "# Sequence models often have such an embedding layer as their first layer. \n",
    "# This layer learns to turn word index sequences into word embedding vectors \n",
    "# during the training process, such that each word index gets mapped to a \n",
    "# dense vector of real values representing that wordâ€™s location in semantic space.\n",
    "if use_pretrained_embedding:\n",
    "    model.add(Embedding(input_dim=num_features,\n",
    "                        output_dim=embedding_dim,\n",
    "                        input_length=input_shape[0],\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=is_embedding_trainable))\n",
    "else:\n",
    "    model.add(Embedding(input_dim=num_features,\n",
    "                        output_dim=embedding_dim,\n",
    "                        input_length=input_shape[0]))\n",
    "\n",
    "for _ in range(blocks-1):\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(SeparableConv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "model.add(SeparableConv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          padding='same'))\n",
    "model.add(SeparableConv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          padding='same'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(rate=dropout_rate))\n",
    "model.add(Dense(op_units, activation=op_activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/1000\n",
      " - 93s - loss: 1.1958 - acc: 0.5329 - val_loss: 1.0860 - val_acc: 0.5685\n",
      "Epoch 2/1000\n",
      " - 93s - loss: 0.9923 - acc: 0.6069 - val_loss: 0.9183 - val_acc: 0.6303\n",
      "Epoch 3/1000\n",
      " - 89s - loss: 0.8414 - acc: 0.6619 - val_loss: 0.8484 - val_acc: 0.6547\n",
      "Epoch 4/1000\n",
      " - 89s - loss: 0.7798 - acc: 0.6830 - val_loss: 0.8336 - val_acc: 0.6596\n",
      "Epoch 5/1000\n"
     ]
    }
   ],
   "source": [
    "# Compile model with learning parameters.\n",
    "if num_classes == 2:\n",
    "    loss = 'binary_crossentropy'\n",
    "else:\n",
    "    loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "# Create callback for early stopping on validation loss. If the loss does\n",
    "# not decrease in two consecutive tries, stop training.\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=2)]\n",
    "\n",
    "# Train and validate model.\n",
    "history = model.fit(\n",
    "          x_train,\n",
    "          train_labels,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_val, val_labels),\n",
    "          verbose=2,  # Logs once per epoch.\n",
    "          batch_size=batch_size)\n",
    "\n",
    "# Print results.\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "# Save model.\n",
    "model.save('rotten_tomatoes_sepcnn_model.h5')\n",
    "print(history['val_acc'][-1], history['val_loss'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
